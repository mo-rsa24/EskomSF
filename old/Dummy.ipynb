{
 "cells": [
  {
   "cell_type": "code",
   "id": "ee4f6f49-905e-465f-9f0b-6e6f434d17b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:18:43.873070Z",
     "start_time": "2025-03-25T04:18:38.626524Z"
    }
   },
   "source": [
    "try:\n",
    "# Import necessary libraries\n",
    "    %pip install pmdarima\n",
    "    from pmdarima import auto_arima\n",
    "    # %pip install statsmodels\n",
    "    from itertools import chain, combinations\n",
    "    from pyspark.sql import SparkSession\n",
    "    # from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from datetime import datetime\n",
    "    from pyspark.dbutils import DBUtils\n",
    "    import py4j\n",
    "    import re\n",
    "    import pyodbc\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "    from pyspark.sql.functions import from_json, col, explode,collect_list\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    \n",
    "    # from pmdarima import auto_arima\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from datetime import datetime\n",
    "    from pyspark.dbutils import DBUtils\n",
    "    import re\n",
    "\n",
    "    import pyodbc\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    from pyspark.sql.types import ArrayType, StructType, StructField, StringType \n",
    "    from pyspark.sql.functions import from_json, col, explode,collect_list\n",
    "except Exception as e:\n",
    "    error_message = str(e)\n",
    "    #print(f\"There is an issue during library instalation : {error_message}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pmdarima in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (2.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from pmdarima) (1.4.2)\n",
      "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from pmdarima) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from pmdarima) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.19 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from pmdarima) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from pmdarima) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from pmdarima) (1.15.2)\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from pmdarima) (0.14.4)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from pmdarima) (2.3.0)\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from pmdarima) (75.8.0)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from pmdarima) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from pandas>=0.19->pmdarima) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from pandas>=0.19->pmdarima) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from pandas>=0.19->pmdarima) (2025.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from scikit-learn>=0.22->pmdarima) (3.6.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from statsmodels>=0.13.2->pmdarima) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\molef\\anaconda3\\envs\\data_science\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "41f0c40d-639f-4275-87e7-b2dba74c2e9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:18:43.904850Z",
     "start_time": "2025-03-25T04:18:43.885117Z"
    }
   },
   "source": [
    "def wrapper_function(fn):\n",
    "    def wrapped():\n",
    "        print(\"Before function runs\")\n",
    "        result = fn()\n",
    "        print(\"After function runs\")\n",
    "        print(\"----------------\")\n",
    "        return result\n",
    "    return wrapped\n",
    "\n",
    "def say_hi():\n",
    "    message = \"Hi!!\"\n",
    "    print(message)\n",
    "    return message\n",
    "\n",
    "wrapped_hi = wrapper_function(say_hi)\n",
    "wrapped_hi()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before function runs\n",
      "Hi!!\n",
      "After function runs\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi!!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "8cf459de-341f-45fe-9691-eb31350ff591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:18:45.070893Z",
     "start_time": "2025-03-25T04:18:45.054175Z"
    }
   },
   "source": [
    "def log_decorator(fn):\n",
    "    def wrapper():\n",
    "        print(f\"Calling function: {fn.__name__}\")\n",
    "        return fn()\n",
    "    return wrapper\n",
    "\n",
    "@log_decorator\n",
    "def greet():\n",
    "    message = \"Hello!!\"\n",
    "    print(message)\n",
    "    print(\"----------------\")\n",
    "    return message\n",
    "\n",
    "greet()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling function: greet\n",
      "Hello!!\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello!!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "c02d7804-9506-4df9-8c42-037f78934494",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:24:17.497944Z",
     "start_time": "2025-03-25T04:24:17.482748Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "# SQL Server Details\n",
    "server_name = \"jdbc:sqlserver://fortrack-maz-sdb-san-dev-01.database.windows.net\"\n",
    "database_name = \"FortrackDB\"\n",
    "url = f\"{server_name};databaseName={database_name};\"\n",
    "\n",
    "# SQL Server Tables\n",
    "table_DBT = \"dbo.DataBrickTasks\"\n",
    "table_UFM = \"dbo.UserForecastMethod\"\n",
    "table_actual = \"dbo.ActualData\"\n",
    "table_version = \"dbo.DimVersion\"\n",
    "table_forecast = \"dw.ForecastActive\"\n",
    "performance_metrics_table = \"dbo.StatisticalPerformanceMetrics\"\n",
    "target_table_name = \"dbo.ForecastFact\"\n",
    "\n",
    "# Authentication (Use environment variables for security)\n",
    "user = os.getenv(\"DB_USER\", \"fortrackSQL\")\n",
    "password = os.getenv(\"DB_PASSWORD\", \"vuxpapyvu@2024\")\n",
    "\n",
    "# JDBC Driver\n",
    "jdbc_driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "8e6f2819-e557-4086-bea7-d249770cb34e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:24:18.072407Z",
     "start_time": "2025-03-25T04:24:17.695074Z"
    }
   },
   "source": [
    "import pyodbc\n",
    "conn_str = (\n",
    "    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    \"SERVER=fortrack-maz-sdb-san-dev-01.database.windows.net;\"\n",
    "    \"DATABASE=FortrackDB;\"\n",
    "    \"UID=fortrackSQL;\"\n",
    "    \"PWD=vuxpapyvu@2024;\"\n",
    ")\n",
    "conn = pyodbc.connect(conn_str)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "0572990c-f406-4e23-b05c-25a1feccbef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T04:24:18.211598Z",
     "start_time": "2025-03-25T04:24:18.121625Z"
    }
   },
   "source": [
    "# Create a cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Test the connection\n",
    "cursor.execute(\"SELECT TOP 5 * FROM dbo.DataBrickTasks\")\n",
    "rows = cursor.fetchall()\n",
    "print(\"ROWS: \", rows)\n",
    "# Print the results\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROWS:  [(1, 'Failed', datetime.datetime(2025, 1, 14, 16, 43, 50, 900000), datetime.datetime(2025, 1, 14, 16, 44, 6, 137000), datetime.datetime(2025, 1, 14, 17, 0, 58, 593000), 137, True, \"NameError: name 'selected_columns' is not defined\"), (2, 'Completed', datetime.datetime(2025, 1, 14, 16, 44, 53, 683000), datetime.datetime(2025, 1, 14, 17, 1, 4, 457000), datetime.datetime(2025, 1, 14, 19, 50, 19, 957000), 138, True, None), (3, 'Failed', datetime.datetime(2025, 1, 15, 11, 0, 23, 680000), datetime.datetime(2025, 1, 15, 11, 1, 5, 780000), datetime.datetime(2025, 1, 15, 11, 14, 1, 610000), 139, True, \"<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 12) (10.139.64.4 executor 0): com.microsoft.sqlserver.jdbc.SQLServerException: The conversion of the varchar value '2150177646' overflowed an int column.\\n\\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFr\"), (4, 'Failed', datetime.datetime(2025, 1, 15, 13, 50, 47, 290000), datetime.datetime(2025, 1, 15, 13, 51, 5, 220000), datetime.datetime(2025, 1, 15, 14, 5, 1, 113000), 140, True, \"<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 12) (10.139.64.4 executor 0): com.microsoft.sqlserver.jdbc.SQLServerException: The conversion of the varchar value '2150177646' overflowed an int column.\\n\\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFr\"), (5, 'Failed', datetime.datetime(2025, 1, 15, 18, 41, 0, 710000), datetime.datetime(2025, 1, 15, 18, 42, 5, 240000), datetime.datetime(2025, 1, 15, 18, 53, 38, 930000), 141, True, \"<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o642.getResult.\\n: org.apache.spark.SparkException: Exception thrown in awaitResult: Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 12) (10.139.64.4 executor 0): com.microsoft.sqlserver.jdbc.SQLServerException: The conversion of the varchar value '2150177646' overflowed an int column.\\n\\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeF\")]\n",
      "(1, 'Failed', datetime.datetime(2025, 1, 14, 16, 43, 50, 900000), datetime.datetime(2025, 1, 14, 16, 44, 6, 137000), datetime.datetime(2025, 1, 14, 17, 0, 58, 593000), 137, True, \"NameError: name 'selected_columns' is not defined\")\n",
      "(2, 'Completed', datetime.datetime(2025, 1, 14, 16, 44, 53, 683000), datetime.datetime(2025, 1, 14, 17, 1, 4, 457000), datetime.datetime(2025, 1, 14, 19, 50, 19, 957000), 138, True, None)\n",
      "(3, 'Failed', datetime.datetime(2025, 1, 15, 11, 0, 23, 680000), datetime.datetime(2025, 1, 15, 11, 1, 5, 780000), datetime.datetime(2025, 1, 15, 11, 14, 1, 610000), 139, True, \"<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 12) (10.139.64.4 executor 0): com.microsoft.sqlserver.jdbc.SQLServerException: The conversion of the varchar value '2150177646' overflowed an int column.\\n\\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFr\")\n",
      "(4, 'Failed', datetime.datetime(2025, 1, 15, 13, 50, 47, 290000), datetime.datetime(2025, 1, 15, 13, 51, 5, 220000), datetime.datetime(2025, 1, 15, 14, 5, 1, 113000), 140, True, \"<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 12) (10.139.64.4 executor 0): com.microsoft.sqlserver.jdbc.SQLServerException: The conversion of the varchar value '2150177646' overflowed an int column.\\n\\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFr\")\n",
      "(5, 'Failed', datetime.datetime(2025, 1, 15, 18, 41, 0, 710000), datetime.datetime(2025, 1, 15, 18, 42, 5, 240000), datetime.datetime(2025, 1, 15, 18, 53, 38, 930000), 141, True, \"<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o642.getResult.\\n: org.apache.spark.SparkException: Exception thrown in awaitResult: Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 12) (10.139.64.4 executor 0): com.microsoft.sqlserver.jdbc.SQLServerException: The conversion of the varchar value '2150177646' overflowed an int column.\\n\\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeF\")\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38f376df-d335-4b2c-91ef-9ec5fba73d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Import necessary libraries\n",
    "    from pyspark.sql import SparkSession\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.dates as mdates\n",
    "    from itertools import chain, combinations\n",
    "    import seaborn as sns\n",
    "    import calendar\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from datetime import datetime\n",
    "    from pyspark.dbutils import DBUtils\n",
    "    import re\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    import pyodbc\n",
    "    import  py4j\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np \n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.functions import from_json, col, explode,collect_list\n",
    "\n",
    "except Exception as e:\n",
    "    error_message = str(e)\n",
    "    #print(f\"There is an issue during library instalation : {error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e340585f-d1b3-4925-9e0c-060c7f0b0994",
   "metadata": {},
   "outputs": [],
   "source": [
    "databrick_task_id=39\n",
    "query = f\"\"\"\n",
    "SELECT TOP 1\n",
    "    ufm.StartDate,\n",
    "    ufm.EndDate,\n",
    "    ufm.Parameters,\n",
    "    ufm.Region,\n",
    "    ufm.Status,\n",
    "    ufm.ForecastMethodID,\n",
    "    ufm.UserForecastMethodID,\n",
    "    ufm.JSONCustomer as CustomerJSON,\n",
    "    ufm.varJSON,  \n",
    "    dfm.Method,\n",
    "    dbt.DatabrickID\n",
    "FROM \n",
    "    [dbo].[DataBrickTasks] AS dbt\n",
    "INNER JOIN \n",
    "    [dbo].[UserForecastMethod] AS ufm ON dbt.UserForecastMethodID = ufm.UserForecastMethodID\n",
    "INNER JOIN \n",
    "    [dbo].[DimForecastMethod] AS dfm ON ufm.ForecastMethodID = dfm.ForecastMethodID\n",
    "\n",
    "WHERE  dbt.DatabrickID={databrick_task_id}\n",
    "ORDER BY\n",
    "    dbt.CreationDate\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a347692-3a7a-47d6-b3eb-61f42fa0acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e0ca673-c9b5-4883-8c79-adee725850ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\molef\\AppData\\Local\\Temp\\ipykernel_12156\\4019385191.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Region</th>\n",
       "      <th>Status</th>\n",
       "      <th>ForecastMethodID</th>\n",
       "      <th>UserForecastMethodID</th>\n",
       "      <th>CustomerJSON</th>\n",
       "      <th>varJSON</th>\n",
       "      <th>Method</th>\n",
       "      <th>DatabrickID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2026-03-31</td>\n",
       "      <td>(6,3,1)(6,3,2,3)</td>\n",
       "      <td>EC</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>{\"CustomerID\": [\"7999245034\",\"7397925811\"]}</td>\n",
       "      <td>{\"VariableID\": [\"OffPeakConsumption\",\"PeakCons...</td>\n",
       "      <td>SARIMA</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    StartDate     EndDate        Parameters Region     Status  \\\n",
       "0  2025-04-01  2026-03-31  (6,3,1)(6,3,2,3)     EC  Completed   \n",
       "\n",
       "   ForecastMethodID  UserForecastMethodID  \\\n",
       "0                 2                    64   \n",
       "\n",
       "                                  CustomerJSON  \\\n",
       "0  {\"CustomerID\": [\"7999245034\",\"7397925811\"]}   \n",
       "\n",
       "                                             varJSON  Method  DatabrickID  \n",
       "0  {\"VariableID\": [\"OffPeakConsumption\",\"PeakCons...  SARIMA           39  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = pyodbc.connect(conn_str)\n",
    "df = pd.read_sql(query, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d16dda05-f3f6-4f85-b7e9-95f3cf189833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SARIMA', '(6,3,1)(6,3,2,3)', 64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Forecast_Method_Name = df.iloc[0]['Method']\n",
    "Model_Parmeters=df.iloc[0]['Parameters']\n",
    "UFMID=df.iloc[0]['UserForecastMethodID']\n",
    "# CustomerID=df.select(\"Customer\").toPandas().iloc[0]['Customer']\n",
    "\n",
    "StartDate=df.iloc[0]['StartDate']\n",
    "EndDate=df.iloc[0]['EndDate']\n",
    "DatabrickID=df.iloc[0]['DatabrickID']\n",
    "Hyper_Parameters=df.iloc[0]['Parameters']\n",
    "Forecast_Method_Name, Model_Parmeters,UFMID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "280ecb1b-a572-4595-92f8-2e836b0056b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {\"CustomerID\": [\"7999245034\",\"7397925811\"]}\n",
       "Name: CustomerJSON, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CustomerJSON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d38e2de-118d-497c-9576-f9f3f497fee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON List:  dict_items([('CustomerID', ['7999245034', '7397925811'])])\n",
      "Entry:  ['7999245034', '7397925811']\n",
      "Comma-separated Customer IDs:\n",
      "'7397925811','7999245034'\n"
     ]
    }
   ],
   "source": [
    "# Check if column exists\n",
    "if \"CustomerJSON\" in df.columns:\n",
    "    customer_ids = []\n",
    "\n",
    "    # Iterate through each row\n",
    "    for row in df[\"CustomerJSON\"].dropna():\n",
    "        try:\n",
    "            json_list = json.loads(row)\n",
    "            print(\"JSON List: \", json_list.items())\n",
    "            for entry in json_list:\n",
    "                print(\"Entry: \",json_list.get(entry))\n",
    "                customer_ids.extend(json_list.get(entry))\n",
    "        except json.JSONDecodeError:\n",
    "            continue  # skip invalid JSON\n",
    "\n",
    "    # Remove any None values and duplicates\n",
    "    customer_ids = [cid for cid in customer_ids if cid]\n",
    "    customer_ids = list(set(customer_ids))\n",
    "\n",
    "    if customer_ids:\n",
    "        multiple_customer_ids_str = \",\".join([f\"'{cid}'\" for cid in customer_ids])\n",
    "    else:\n",
    "        multiple_customer_ids_str = ''\n",
    "    \n",
    "    print(\"Comma-separated Customer IDs:\")\n",
    "    print(multiple_customer_ids_str)\n",
    "else:\n",
    "    print(\"Column 'CustomerJSON' does not exist in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d32eca4c-4f59-4346-a9a4-c70402836404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed:  {'VariableID': ['OffPeakConsumption', 'PeakConsumption', 'StandardConsumption', 'Block1Consumption', 'Block2Consumption', 'Block3Consumption', 'Block4Consumption', 'NonTOUConsumption']}\n",
      "All VariableIDs as comma-separated string:\n",
      "Block2Consumption,PeakConsumption,OffPeakConsumption,Block1Consumption,NonTOUConsumption,StandardConsumption,Block3Consumption,Block4Consumption\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if \"varJSON\" in df.columns:\n",
    "        variable_ids = []\n",
    "\n",
    "        for row in df[\"varJSON\"].dropna():\n",
    "            try:\n",
    "                parsed = json.loads(row)\n",
    "                print(\"Parsed: \", parsed)\n",
    "                vars_list = parsed.get(\"VariableID\", [])\n",
    "                if vars_list:\n",
    "                    variable_ids.extend(vars_list)\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # skip invalid JSON rows\n",
    "\n",
    "        # Clean up: remove duplicates and empty entries\n",
    "        variable_ids = [v for v in variable_ids if v]\n",
    "        variable_ids = list(set(variable_ids))\n",
    "\n",
    "        if variable_ids:\n",
    "            all_variables = \",\".join(variable_ids)\n",
    "        else:\n",
    "            all_variables = \"\"\n",
    "\n",
    "        print(\"All VariableIDs as comma-separated string:\")\n",
    "        print(all_variables)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Column 'varJSON' does not exist in the dataframe.\")\n",
    "except ValueError as e:\n",
    "    error_message = str(e)\n",
    "    raise ValueError(\"Column 'varJSON' does not exist in the dataframe or schema is invalid.\")\n",
    "except Exception as e:\n",
    "    error_message = str(e)\n",
    "    raise RuntimeError(f\"Unexpected error occurred: {error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9333d50a-77b2-411d-bcd1-e63ea1f3baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all_variables is a list\n",
    "if isinstance(all_variables, str):\n",
    "    all_variables = all_variables.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99c2e2ff-3d9f-4b6f-88c2-1842357b652f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Block2Consumption',\n",
       " 'PeakConsumption',\n",
       " 'OffPeakConsumption',\n",
       " 'Block1Consumption',\n",
       " 'NonTOUConsumption',\n",
       " 'StandardConsumption',\n",
       " 'Block3Consumption',\n",
       " 'Block4Consumption']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72408dea-2117-4790-b67d-16452f11fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prediction_columns = [\"PeakConsumption\", \"StandardConsumption\", \"OffPeakConsumption\",\"Block1Consumption\", \"Block2Consumption\",\"Block3Consumption\", \"Block4Consumption\",     \"NonTOUConsumption\"]\n",
    "\n",
    "all_combination = chain.from_iterable(combinations(all_prediction_columns, r) for r in range(1,len(all_prediction_columns)+1))\n",
    "columns_mapping = {}\n",
    "for comb in all_combination:\n",
    "    comb_set = frozenset(comb)\n",
    "    comb_value = ['ReportingMonth', 'CustomerID'] + list(comb)\n",
    "    columns_mapping[comb_set] = comb_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9756de4-8980-4e1d-90dd-384d6b3e962e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total combinations: 255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Print the result for testing\n",
    "print(f\"✅ Total combinations: {len(columns_mapping)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d4a9347-b2f2-4700-a5f2-f7bfdf5fde2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Sample mappings:\n",
      "{'Block3Consumption', 'StandardConsumption', 'Block1Consumption', 'OffPeakConsumption'} -> ['ReportingMonth', 'CustomerID', 'StandardConsumption', 'OffPeakConsumption', 'Block1Consumption', 'Block3Consumption']\n",
      "\n",
      "{'StandardConsumption', 'Block1Consumption'} -> ['ReportingMonth', 'CustomerID', 'StandardConsumption', 'Block1Consumption']\n",
      "\n",
      "{'StandardConsumption', 'Block3Consumption', 'Block4Consumption', 'NonTOUConsumption'} -> ['ReportingMonth', 'CustomerID', 'StandardConsumption', 'Block3Consumption', 'Block4Consumption', 'NonTOUConsumption']\n",
      "\n",
      "{'StandardConsumption', 'NonTOUConsumption'} -> ['ReportingMonth', 'CustomerID', 'StandardConsumption', 'NonTOUConsumption']\n",
      "\n",
      "{'Block2Consumption', 'StandardConsumption', 'Block3Consumption', 'Block4Consumption'} -> ['ReportingMonth', 'CustomerID', 'StandardConsumption', 'Block2Consumption', 'Block3Consumption', 'Block4Consumption']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print 5 random combinations\n",
    "import random\n",
    "sample_keys = random.sample(list(columns_mapping.keys()), 5)\n",
    "\n",
    "print(\"🔍 Sample mappings:\")\n",
    "for k in sample_keys:\n",
    "    print(f\"{set(k)} -> {columns_mapping[k]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "107570e9-0e21-43fb-983f-1addc9aad611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Block1Consumption',\n",
       " 'Block2Consumption',\n",
       " 'Block3Consumption',\n",
       " 'Block4Consumption',\n",
       " 'NonTOUConsumption',\n",
       " 'OffPeakConsumption',\n",
       " 'PeakConsumption',\n",
       " 'StandardConsumption'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variables_set = set(all_variables)\n",
    "all_variables_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f182a8b2-c829-4564-96e0-099256ae7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"All selected variables :\", str(all_variables_set))\n",
    "\n",
    "# Find the matching key in the columns_mapping\n",
    "matching_key = None\n",
    "# #print (\"all_variables_set :\", all_variables_set )\n",
    "for key in columns_mapping.keys():\n",
    "    # #print(key)\n",
    "    # #print (\"current key from columns mapping :\", key )\n",
    "    if key.issuperset(all_variables_set):\n",
    "        \n",
    "        matching_key = key\n",
    "        break\n",
    "\n",
    "# Select the appropriate columns based on the matching key\n",
    "if matching_key:\n",
    "    # #print(matching_key)\n",
    "    selected_columns = columns_mapping[matching_key]\n",
    "\n",
    "    #print(f\" All columns based on selectedcolumns{selected_columns}\")\n",
    "else:\n",
    "    print(\"No matching columns found in AllVariables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1fb1900-9d97-4a4a-ac12-f6370e85b124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ReportingMonth',\n",
       " 'CustomerID',\n",
       " 'PeakConsumption',\n",
       " 'StandardConsumption',\n",
       " 'OffPeakConsumption',\n",
       " 'Block1Consumption',\n",
       " 'Block2Consumption',\n",
       " 'Block3Consumption',\n",
       " 'Block4Consumption',\n",
       " 'NonTOUConsumption']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3104cec0-dcde-487d-87d2-7d12053091c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\molef\\AppData\\Local\\Temp\\ipykernel_12156\\2184366618.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserForecastMethodID</th>\n",
       "      <th>PodID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>TariffID</th>\n",
       "      <th>ReportingMonth</th>\n",
       "      <th>PeakConsumption</th>\n",
       "      <th>StandardConsumption</th>\n",
       "      <th>OffPeakConsumption</th>\n",
       "      <th>Block1Consumption</th>\n",
       "      <th>Block2Consumption</th>\n",
       "      <th>Block3Consumption</th>\n",
       "      <th>Block4Consumption</th>\n",
       "      <th>NonTOUConsumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0736171562</td>\n",
       "      <td>7397925811</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>141734.0</td>\n",
       "      <td>409040.0</td>\n",
       "      <td>647244.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0736171562</td>\n",
       "      <td>7397925811</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>2021-05-01</td>\n",
       "      <td>148250.0</td>\n",
       "      <td>382914.0</td>\n",
       "      <td>561082.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>0736171562</td>\n",
       "      <td>7397925811</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>153818.0</td>\n",
       "      <td>394294.0</td>\n",
       "      <td>552320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>0736171562</td>\n",
       "      <td>7397925811</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>182204.0</td>\n",
       "      <td>455822.0</td>\n",
       "      <td>659894.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>0736171562</td>\n",
       "      <td>7397925811</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>2021-08-01</td>\n",
       "      <td>144534.0</td>\n",
       "      <td>388444.0</td>\n",
       "      <td>621796.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserForecastMethodID       PodID  CustomerID  TariffID ReportingMonth  \\\n",
       "0                    64  0736171562  7397925811  MEGAFLEX     2021-04-01   \n",
       "1                    64  0736171562  7397925811  MEGAFLEX     2021-05-01   \n",
       "2                    64  0736171562  7397925811  MEGAFLEX     2021-06-01   \n",
       "3                    64  0736171562  7397925811  MEGAFLEX     2021-07-01   \n",
       "4                    64  0736171562  7397925811  MEGAFLEX     2021-08-01   \n",
       "\n",
       "   PeakConsumption  StandardConsumption  OffPeakConsumption  \\\n",
       "0         141734.0             409040.0            647244.0   \n",
       "1         148250.0             382914.0            561082.0   \n",
       "2         153818.0             394294.0            552320.0   \n",
       "3         182204.0             455822.0            659894.0   \n",
       "4         144534.0             388444.0            621796.0   \n",
       "\n",
       "   Block1Consumption  Block2Consumption  Block3Consumption  Block4Consumption  \\\n",
       "0                0.0                0.0                0.0                0.0   \n",
       "1                0.0                0.0                0.0                0.0   \n",
       "2                0.0                0.0                0.0                0.0   \n",
       "3                0.0                0.0                0.0                0.0   \n",
       "4                0.0                0.0                0.0                0.0   \n",
       "\n",
       "   NonTOUConsumption  \n",
       "0                0.0  \n",
       "1                0.0  \n",
       "2                0.0  \n",
       "3                0.0  \n",
       "4                0.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"SELECT * from dbo.PredictiveInputData(64)\"\n",
    "conn = pyodbc.connect(conn_str)\n",
    "df = pd.read_sql(query, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e4636b3-7e06-41ff-908c-a0c686122916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   UserForecastMethodID  175 non-null    int64  \n",
      " 1   PodID                 175 non-null    object \n",
      " 2   CustomerID            175 non-null    object \n",
      " 3   TariffID              175 non-null    object \n",
      " 4   ReportingMonth        175 non-null    object \n",
      " 5   PeakConsumption       175 non-null    float64\n",
      " 6   StandardConsumption   175 non-null    float64\n",
      " 7   OffPeakConsumption    175 non-null    float64\n",
      " 8   Block1Consumption     175 non-null    float64\n",
      " 9   Block2Consumption     175 non-null    float64\n",
      " 10  Block3Consumption     175 non-null    float64\n",
      " 11  Block4Consumption     175 non-null    float64\n",
      " 12  NonTOUConsumption     175 non-null    float64\n",
      "dtypes: float64(8), int64(1), object(4)\n",
      "memory usage: 17.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05fe9e64-d2d0-47e6-a807-8b723bfde34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserForecastMethodID</th>\n",
       "      <th>PodID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>TariffID</th>\n",
       "      <th>ReportingMonth</th>\n",
       "      <th>PeakConsumption</th>\n",
       "      <th>StandardConsumption</th>\n",
       "      <th>OffPeakConsumption</th>\n",
       "      <th>Block1Consumption</th>\n",
       "      <th>Block2Consumption</th>\n",
       "      <th>Block3Consumption</th>\n",
       "      <th>Block4Consumption</th>\n",
       "      <th>NonTOUConsumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0736171562</td>\n",
       "      <td>7397925811</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>141734.0</td>\n",
       "      <td>409040.0</td>\n",
       "      <td>647244.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0736171562</td>\n",
       "      <td>7397925811</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>2021-05-01</td>\n",
       "      <td>148250.0</td>\n",
       "      <td>382914.0</td>\n",
       "      <td>561082.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>0736171562</td>\n",
       "      <td>7397925811</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>153818.0</td>\n",
       "      <td>394294.0</td>\n",
       "      <td>552320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>0736171562</td>\n",
       "      <td>7397925811</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>182204.0</td>\n",
       "      <td>455822.0</td>\n",
       "      <td>659894.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>0736171562</td>\n",
       "      <td>7397925811</td>\n",
       "      <td>MEGAFLEX</td>\n",
       "      <td>2021-08-01</td>\n",
       "      <td>144534.0</td>\n",
       "      <td>388444.0</td>\n",
       "      <td>621796.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserForecastMethodID       PodID  CustomerID  TariffID ReportingMonth  \\\n",
       "0                    64  0736171562  7397925811  MEGAFLEX     2021-04-01   \n",
       "1                    64  0736171562  7397925811  MEGAFLEX     2021-05-01   \n",
       "2                    64  0736171562  7397925811  MEGAFLEX     2021-06-01   \n",
       "3                    64  0736171562  7397925811  MEGAFLEX     2021-07-01   \n",
       "4                    64  0736171562  7397925811  MEGAFLEX     2021-08-01   \n",
       "\n",
       "   PeakConsumption  StandardConsumption  OffPeakConsumption  \\\n",
       "0         141734.0             409040.0            647244.0   \n",
       "1         148250.0             382914.0            561082.0   \n",
       "2         153818.0             394294.0            552320.0   \n",
       "3         182204.0             455822.0            659894.0   \n",
       "4         144534.0             388444.0            621796.0   \n",
       "\n",
       "   Block1Consumption  Block2Consumption  Block3Consumption  Block4Consumption  \\\n",
       "0                0.0                0.0                0.0                0.0   \n",
       "1                0.0                0.0                0.0                0.0   \n",
       "2                0.0                0.0                0.0                0.0   \n",
       "3                0.0                0.0                0.0                0.0   \n",
       "4                0.0                0.0                0.0                0.0   \n",
       "\n",
       "   NonTOUConsumption  \n",
       "0                0.0  \n",
       "1                0.0  \n",
       "2                0.0  \n",
       "3                0.0  \n",
       "4                0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Optional sorting ---\n",
    "df = df.sort_values(by=[\"PodID\", \"ReportingMonth\"])\n",
    "\n",
    "# --- Type conversion ---\n",
    "df['CustomerID'] = df['CustomerID'].astype(str)\n",
    "df['ReportingMonth'] = pd.to_datetime(\n",
    "    df['ReportingMonth'], format='%Y-%m-%d'\n",
    ").dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# --- Display top rows ---\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12421142-d573-43e7-815e-9fc3d5f39f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4cb3b47b-a074-4860-85a3-06d6f3770fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future consumption will be predicted for customers:['7397925811' '7999245034']\n"
     ]
    }
   ],
   "source": [
    "multiple_customer_ids_list = pandas_df['CustomerID'].unique()\n",
    "if len(multiple_customer_ids_list)>0:\n",
    "    # Output the comma-separated IDs\n",
    "    print(f\"Future consumption will be predicted for customers:{multiple_customer_ids_list}\")\n",
    "else:\n",
    "    # Output the comma-separated IDs\n",
    "    print(f\"No consumption data available for selected Customer IDs: {multiple_customer_ids_list} or customer ids selection is not successful\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if len(pandas_df) == 0:\n",
    "    print(\"No data found for these customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dd3cfb31-9ae2-47b7-9cd9-ca9d3ea6e193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-02-01 00:00:00')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most recent reporting month in the data, which will be used to determine the starting point for forecasting.\n",
    "Actuals_last_date = pandas_df['ReportingMonth'].max() \n",
    "Actuals_last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bc9ae1b7-f65d-4e9c-9844-e0b74caf7cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2025, 4, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StartDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a855975-507e-4953-828c-fdab93122e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a date range from the start date to the end date with a monthly frequency, starting on the first of each month.\n",
    "# This range represents the forecast period.\n",
    "\n",
    "forecast_dates = pd.date_range(start=StartDate, end=EndDate, freq='MS')[0:]\n",
    "#print(forecast_dates,f\"Actual_last_date {Actuals_last_date}\",f\"No of month to pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "29cdfd79-ba72-40bc-a29f-f54fbfee520d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2025-04-01 00:00:00'),\n",
       " Timestamp('2025-05-01 00:00:00'),\n",
       " Timestamp('2025-06-01 00:00:00'),\n",
       " Timestamp('2025-07-01 00:00:00'),\n",
       " Timestamp('2025-08-01 00:00:00'),\n",
       " Timestamp('2025-09-01 00:00:00'),\n",
       " Timestamp('2025-10-01 00:00:00'),\n",
       " Timestamp('2025-11-01 00:00:00'),\n",
       " Timestamp('2025-12-01 00:00:00'),\n",
       " Timestamp('2026-01-01 00:00:00'),\n",
       " Timestamp('2026-02-01 00:00:00'),\n",
       " Timestamp('2026-03-01 00:00:00')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(forecast_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4e0aff7e-c7b2-4d87-b8e4-5008b9386643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['7397925811', '7999245034'], dtype=object),\n",
       " array(['0736171562', '3289712697', '3750837137', '7027193072',\n",
       "        '9849938574'], dtype=object))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate if data is only processed for intended customer\n",
    "\n",
    "unique_customers = pandas_df['CustomerID'].unique()\n",
    "unique_PodIDs = pandas_df['PodID'].unique()\n",
    "\n",
    "unique_customers, unique_PodIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a8cab5b1-d6ec-454c-919b-ac231ad9b53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SARIMA'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Forecast_Method_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4e9b4d8d-2469-439c-8684-956c76a1d8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(6,3,1)(6,3,2,3)'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hyper_Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5c2b0584-ea57-4c98-9ea1-747f6a3ce0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('(6,3,1)', '(6,3,2,3)')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order, seasonal_order = re.findall(r'\\(.*?\\)',Hyper_Parameters)   #['(1,1,1),(1,1,1,2)']\n",
    "order, seasonal_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b589f6b6-af6f-4d87-8043-2fbfe25351ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove parentheses and split the parameters by commas to extract individual elements.\n",
    "order = order.strip('()')\n",
    "order_parameters = order.split(',')\n",
    "\n",
    "seasonal_order=seasonal_order.strip('()')\n",
    "seasonal_order_parameters = seasonal_order.split(',')\n",
    "\n",
    "\n",
    "\n",
    "# Assign the values to the variables\n",
    "p = int(order_parameters[0])\n",
    "d = int(order_parameters[1])\n",
    "q = int(order_parameters[2])\n",
    "\n",
    "\n",
    "# Convert extracted string parameters into integers and assign them to corresponding variables. \n",
    " \n",
    "s_p = int(seasonal_order_parameters[0])\n",
    "s_d = int(seasonal_order_parameters[1])\n",
    "s_q = int(seasonal_order_parameters[2])\n",
    "s_m = int(seasonal_order_parameters[3])\n",
    "\n",
    "#print(f\"(Order_Paramaters {p,d,q}  Seasonal_Paramaters {s_p,s_d,s_q,s_m}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b948e83f-f5bc-4527-8489-c4cd39e677e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6', '3', '1']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4f8b4b7b-fd97-430b-a73c-d28388d24583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PeakConsumption',\n",
       " 'StandardConsumption',\n",
       " 'OffPeakConsumption',\n",
       " 'Block1Consumption',\n",
       " 'Block2Consumption',\n",
       " 'Block3Consumption',\n",
       " 'Block4Consumption',\n",
       " 'NonTOUConsumption']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_columns = selected_columns[2:]   \n",
    "plot_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e40ec4-f95a-449e-aa93-7c08fde054d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
